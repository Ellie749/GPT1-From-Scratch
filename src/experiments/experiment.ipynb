{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "dataset = open('dataset.txt', 'r', encoding='utf-8').read()\n",
    "print(len(dataset))\n",
    "print(dataset[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset character level dictionary is: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Length of the dictionary is: 65\n"
     ]
    }
   ],
   "source": [
    "# Find tokens\n",
    "\n",
    "chars = sorted(set(dataset))\n",
    "dictionary = ''.join(chars)\n",
    "print(f\"Dataset character level dictionary is: {dictionary}\")\n",
    "print(f\"Length of the dictionary is: {len(dictionary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "char_id = {s:i for i, s in enumerate(dictionary)}\n",
    "id_char = {i:s for i, s in enumerate(char_id)}\n",
    "encoder = lambda s: [char_id[c] for c in s]\n",
    "decode = lambda i: ''.join([id_char[c] for c in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = torch.tensor(encoder(dataset), dtype=torch.long)\n",
    "print(encoded_dataset.shape, encoded_dataset.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n",
      "torch.Size([1003854])\n",
      "torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "train_length = int(0.9 * len(encoded_dataset))\n",
    "print(train_length)\n",
    "train_dataset = encoded_dataset[:train_length]\n",
    "validation_dataset = encoded_dataset[train_length:]\n",
    "print(train_dataset.shape)\n",
    "print(validation_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 8\n",
    "EMBEDDING_DIM = 8\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "VOCAB_SIZE = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training data is: torch.Size([8714, 16, 8])\n",
      "Size of the training label is: torch.Size([8714, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "def make_data(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    d = data.tolist()\n",
    "    for i in range(len(d) // SEQUENCE_LENGTH):\n",
    "        x.append(d[i*8 : (i+1)*8])\n",
    "        y.append(d[i*8 +1 : (i+1)*8 + 1])\n",
    "    #return torch.tensor(x), torch.tensor(y)\n",
    "    return torch.tensor(x).reshape(-1, BATCH_SIZE, SEQUENCE_LENGTH), torch.tensor(y).reshape(-1, BATCH_SIZE, SEQUENCE_LENGTH)\n",
    "\n",
    "X_train, y_train = make_data(encoded_dataset)\n",
    "print(f\"Size of the training data is: {X_train.shape}\")\n",
    "print(f\"Size of the training label is: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have:<br>\n",
    "target = [2, 0, 1]<br>\n",
    "logits = [[0.5, 0.2, 0.3], [0.1, 0.1, 0.8], [0.4, 0.2, 0.4]]<br><br>\n",
    "then cross entropy loss, automatically gets the indice 2 of the first logits probability and 0th index of the second one and so on ... <br>\n",
    "Since our labels are not one hotted, cross entropy does the job automatically for us <br>\n",
    "That's why flattening is okay and is also handled in backpropagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of logits: torch.Size([16, 8, 65])\n",
      "shape of labels: torch.Size([16, 8])\n",
      "loss is: 4.495141506195068\n",
      "model's generated text: &sC$'X?rrv$xws$Jx&'HyXuCu$JqXO-:$$w$$$sWvJTH,$wxusOv-v$Y,hs\n",
      "SA$$vGsUHyQxs$jsxPm?spvvyc$CYX$sPZ'm$xYAz\n"
     ]
    }
   ],
   "source": [
    "# make bigram model\n",
    "\n",
    "class BiGram(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.token_embedding_table(x)\n",
    "    \n",
    "    def calc_loss(self, logits, target):\n",
    "        # logits = self(x)\n",
    "        return F.cross_entropy(logits.view(-1, logits.shape[-1]), target.view(-1))\n",
    "          \n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            logits = self(x)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            x_next = torch.multinomial(probs, 1, replacement=True)[0]\n",
    "            x = torch.cat((x, x_next), dim=0)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=10):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        epoch_loss = 0\n",
    "        for i in range(epochs):\n",
    "            print(f\"[INFO]  epoch {i}, loss: {epoch_loss/X_train.shape[0]}\")\n",
    "            epoch_loss = 0\n",
    "            for b in range(X_train.shape[0]):\n",
    "                logits = self(X_train[b])\n",
    "                loss = self.calc_loss(logits, y_train[b])\n",
    "                epoch_loss+=loss\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print(f\"loss: {loss.item()}\")\n",
    "                \n",
    "        \n",
    "    \n",
    "bigram_model = BiGram(VOCAB_SIZE)\n",
    "logits = bigram_model((X_train[0]))\n",
    "print(f\"shape of logits: {logits.shape}\")\n",
    "print(f\"shape of labels: {y_train[0].shape}\")\n",
    "bigram_model_loss = bigram_model.calc_loss(logits, y_train[0])\n",
    "print(f\"loss is: {bigram_model_loss}\") # if we had to guess randomly the loss would have been -ln(1/65) = 4.18\n",
    "print(f\"model's generated text: {decode(bigram_model.generate(torch.tensor([4]), 100).tolist())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  epoch 0, loss: 0.0\n",
      "[INFO]  epoch 1, loss: 4.168179035186768\n",
      "[INFO]  epoch 2, loss: 3.4456942081451416\n",
      "[INFO]  epoch 3, loss: 2.994004487991333\n",
      "[INFO]  epoch 4, loss: 2.7325074672698975\n",
      "[INFO]  epoch 5, loss: 2.586735486984253\n",
      "[INFO]  epoch 6, loss: 2.5063488483428955\n",
      "[INFO]  epoch 7, loss: 2.4611153602600098\n",
      "[INFO]  epoch 8, loss: 2.434145212173462\n",
      "[INFO]  epoch 9, loss: 2.4170758724212646\n",
      "[INFO]  epoch 10, loss: 2.4058172702789307\n",
      "[INFO]  epoch 11, loss: 2.39817476272583\n",
      "[INFO]  epoch 12, loss: 2.3928632736206055\n",
      "[INFO]  epoch 13, loss: 2.3890953063964844\n",
      "[INFO]  epoch 14, loss: 2.386380434036255\n",
      "[INFO]  epoch 15, loss: 2.384387254714966\n",
      "[INFO]  epoch 16, loss: 2.382903575897217\n",
      "[INFO]  epoch 17, loss: 2.3817837238311768\n",
      "[INFO]  epoch 18, loss: 2.380929708480835\n",
      "[INFO]  epoch 19, loss: 2.3802695274353027\n"
     ]
    }
   ],
   "source": [
    "bigram_model.train(X_train[:1000], y_train[:1000], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's generated text: \n",
      "\n",
      "MOL\n",
      "\n",
      "dBWC\n",
      "TyS\n",
      "H\n",
      "\n",
      "tWbIIVBACMANTTT\n",
      "ACAWLBFYAWSBATBF\n",
      "\n",
      "\n",
      "AWYYW'TSNB\n",
      "\n",
      "OWWWOTy\n",
      "tTAFI\n",
      "BtS\n",
      "WAOVAp\n",
      "A\n",
      "WAAMTO\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "logits = bigram_model((X_train[0]))\n",
    "bigram_model.calc_loss(logits, y_train[0])\n",
    "print(f\"model's generated text: {decode(bigram_model.generate(torch.tensor([0]), 100).tolist())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch_AndrejKarpathy_venv",
   "language": "python",
   "name": "pytorch_andrejkarpathy_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
